{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('nutrition.dat',delim_whitespace=True)\n",
    "df.columns\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Python code for multiple regression model with all factors\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# clean up data frame\n",
    "df.dropna() # drop missing values\n",
    "model1 = sm.ols(formula='CALORIES~WT_GRAMS+PC_WATER+PROTEIN+FAT+SAT_FAT+MONUNSAT+POLUNSAT+CHOLEST+CARBO+CALCIUM+PHOSPHOR+IRON+POTASS+SODIUM+VIT_A_IU+VIT_A_RE+THIAMIN+RIBOFLAV+NIACIN+ASCORBIC+CAL_GRAM+IRN_GRAM+PRO_GRAM+FAT_GRAM',data=df)\n",
    "fitted1 = model1.fit()\n",
    "print(fitted1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3\n",
    "## List factors that can be removed based upon p-values\n",
    "\n",
    "MONUNSAT, POLUNSAT, CHOLEST, VIT_A_IU, VIT_A_RE, RIBOFLAV, NIACIN, ASCORBIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Python code for fitting reduced multiple regression model\n",
    "\n",
    "model2 = sm.ols(formula='CALORIES~WT_GRAMS+PC_WATER+PROTEIN+FAT+SAT_FAT+CARBO+CALCIUM+PHOSPHOR+IRON+POTASS+SODIUM+THIAMIN+CAL_GRAM+IRN_GRAM+PRO_GRAM+FAT_GRAM',data=df)\n",
    "fitted2 = model2.fit()\n",
    "print(fitted2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Python code for prepAnovaTable for the full and reduced models found in cells 2 and 4\n",
    "\n",
    "def prepANOVAtable(model):\n",
    "    tmp={'Source':['Regression','Error','Total'],'df': [model.df_model,model.df_resid,model.df_model+model.df_resid],'Sum of Squares':[model.ssr,model.ess,model.ssr+model.ess],'Mean Square':[model.mse_model,model.mse_resid,model.mse_total],'F Statistic':[model.fvalue,'',''],'p value':[model.f_pvalue,'','']}\n",
    "    anova_df=pd.DataFrame(data=tmp)\n",
    "    anova_df=anova_df[['Source','df','Sum of Squares','Mean Square','F Statistic','p value']]\n",
    "    return(anova_df)\n",
    "    \n",
    "print(\"full model\")\n",
    "anova_full_df=prepANOVAtable(fitted1)\n",
    "print(anova_full_df)\n",
    "print(\"reduced model\")\n",
    "anova_reduced_df=prepANOVAtable(fitted2)\n",
    "print(anova_reduced_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 changed to code\n",
    "## Conduct a partial F-test\n",
    "from scipy.stats import f\n",
    "\n",
    "\n",
    "# Compute the partial F-test\n",
    "rss_full = fitted1.ssr  # Residual sum of squares for full model\n",
    "rss_reduced = fitted2.ssr  # Residual sum of squares for reduced model\n",
    "\n",
    "n = model1.nobs  # Number of observations\n",
    "p_full = len(fitted1.params)-1  # Number of predictors in the full model (including intercept)\n",
    "p_reduced = len(fitted2.params)-1  # Number of predictors in the reduced model (including intercept)\n",
    "\n",
    "df_numerator = p_full - p_reduced  # Degrees of freedom for the numerator\n",
    "df_denominator = n - p_full  # Degrees of freedom for the denominator\n",
    "\n",
    "f_statistic = ((rss_reduced - rss_full) / df_numerator) / (rss_full / df_denominator)\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = 1 - f.cdf(f_statistic, df_numerator, df_denominator)\n",
    "\n",
    "# Print the results\n",
    "print(\"Partial F-Test Results:\")\n",
    "print(f\"F-statistic: {f_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"The additional predictors in the full model significantly improve the model fit.\")\n",
    "else:\n",
    "    print(\"The additional predictors in the full model do not significantly improve the model fit.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Python code for residual analysis of reduced model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residual2=fitted2.resid_pearson\n",
    "fittedValues=fitted2.fittedvalues\n",
    "plt.plot(fittedValues, residual2, 'ro')\n",
    "plt.xlabel('fitted values')\n",
    "plt.ylabel('standardized residual')\n",
    "plt.title('Standardized Residuals versus fitted values')\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "id=np.arange(1,df[\"CALORIES\"].size+1,1)\n",
    "plt.plot(id,residual2,'-ro')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('standardized residual')\n",
    "plt.title('Standardized Residuals versus Index')\n",
    "plt.show()\n",
    "\n",
    "from scipy import stats\n",
    "# normal probability plot\n",
    "stats.probplot(residual2, plot=plt)\n",
    "plt.title('Normal Probability Plot')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(df[\"WT_GRAMS\"], residual2, 'ro')\n",
    "plt.xlabel('WT_GRAMS')\n",
    "plt.ylabel('standardized residual')\n",
    "plt.title('Standardized Residuals versus WT_GRAMS')\n",
    "\n",
    "print(\"ideally there would be residuals plots for each of the terms in the model and those terms not in the model also. That is too many plots for this solution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 8\n",
    "## Interpret the results of the residual analysis\n",
    "\n",
    "**The data is a mess. There are a lot of outliers and the residuals do not appear to be normallly distributed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "# Python code to identify high-leverage and influential points\n",
    "\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "test_class = OLSInfluence(fitted2)\n",
    "plt.stem(np.arange(len(test_class.hat_matrix_diag)),test_class.hat_matrix_diag)\n",
    "plt.title('Diagonal Elements of Hat Matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.stem(np.arange(len(test_class.cooks_distance[0])),test_class.cooks_distance[0])\n",
    "plt.title(\"Cook's Distance\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(test_class.dffits[0])),test_class.dffits[0])\n",
    "plt.title(\"DFFITS\")\n",
    "plt.show()\n",
    "\n",
    "betas=np.array(test_class.dfbetas)\n",
    "betas[:,0]\n",
    "plt.plot(np.arange(len(betas[:,0])),betas[:,0])\n",
    "plt.title(\"DFBETAS for y-intercept\")\n",
    "plt.show()\n",
    "\n",
    "betas=np.array(test_class.dfbetas)\n",
    "betas[:,1]\n",
    "plt.plot(np.arange(len(betas[:,1])),betas[:,1])\n",
    "plt.title(\"DFBETAS for slope term\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 10\n",
    "## List high-leverage and influential points\n",
    "\n",
    "**There are almost too many to list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Python code to perform forward selection of all factors\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "def forward_selection(data, response):\n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "           evaluated by adjusted R-squared\n",
    "    \"\"\"\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_score, best_new_score = 0.0, 0.0\n",
    "    while remaining and current_score == best_new_score:\n",
    "        scores_with_candidates = []\n",
    "        for candidate in remaining:\n",
    "            formula = \"{} ~ {} + 1\".format(response,\n",
    "                                           ' + '.join(selected + [candidate]))\n",
    "            score = smf.ols(formula, data).fit().rsquared_adj\n",
    "            scores_with_candidates.append((score, candidate))\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_candidate = scores_with_candidates.pop()\n",
    "        if current_score < best_new_score:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_score = best_new_score\n",
    "    formula = \"{} ~ {} + 1\".format(response,\n",
    "                                   ' + '.join(selected))\n",
    "    model = smf.ols(formula, data).fit()\n",
    "    return model\n",
    "\n",
    "df_reduced=df.drop(columns=['FOOD'])\n",
    "df_reduced.head()\n",
    "forward=forward_selection(df_reduced,'CALORIES')\n",
    "print(forward.model.formula)\n",
    "print(forward.rsquared)\n",
    "print(forward.rsquared_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 12\n",
    "## Best model found using forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Python code to perform backwards selection of all factors\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    features = X.columns.tolist()\n",
    "    while len(features) > 0:\n",
    "        model = sm.OLS(y, sm.add_constant(X[features])).fit()\n",
    "        p_values = model.pvalues.iloc[1:]  # Ignore the intercept's p-value\n",
    "        max_p_value = p_values.max()\n",
    "        if max_p_value > significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "        else:\n",
    "            break\n",
    "    return features\n",
    "#\n",
    "y = df_reduced['CALORIES']\n",
    "#print(y)\n",
    "X = df_reduced\n",
    "X=X.drop(columns=['CALORIES'])\n",
    "#print(X)\n",
    "#print(X)\n",
    "#print(y)\n",
    "best_features = backward_elimination(X, y)\n",
    "print(\"Selected features:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 14\n",
    "## Best model found using backwards selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "# Python code to perform stepwise selection of all factors\n",
    "\n",
    "def stepwise_selection(X, y, initial_list=[], threshold_in=0.05, threshold_out=0.05):\n",
    "    \"\"\" Perform a stepwise regression.\n",
    "    Args:\n",
    "        X - pandas DataFrame with candidate features\n",
    "        y - list-like with the dependent variable\n",
    "        initial_list - list of features to start with (default is empty)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "    Returns:\n",
    "        list of selected features\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed = False\n",
    "        # Forward step\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(X[included + [new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "            print(f'Add  {best_feature} with p-value {best_pval:.6}')\n",
    "\n",
    "        # Backward step\n",
    "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]  # all except intercept\n",
    "        worst_pval = pvalues.max()\n",
    "        if worst_pval > threshold_out:\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            changed = True\n",
    "            print(f'Remove {worst_feature} with p-value {worst_pval:.6}')\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return included\n",
    "#\n",
    "best_features = stepwise_selection(X, y)\n",
    "print(\"Selected features:\", best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 16\n",
    "## Best model found using stepwise selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "# Python code to perform all subsets selection.\n",
    "\n",
    "import itertools\n",
    "#\n",
    "def best_subsets_selection(X, y):\n",
    "    \"\"\"Perform best subsets regression and return the best model based on AIC, BIC, and Adjusted R².\"\"\"\n",
    "    n = len(X)  # number of observations\n",
    "    all_features = X.columns.tolist()\n",
    "    \n",
    "    # Store results for each subset\n",
    "    results = []\n",
    "    \n",
    "    # Loop over all possible subsets of features\n",
    "    for k in range(1, len(all_features) + 1):\n",
    "        for combo in itertools.combinations(all_features, k):\n",
    "            combo = list(combo)\n",
    "            X_subset = sm.add_constant(X[combo])\n",
    "            model = sm.OLS(y, X_subset).fit()\n",
    "            \n",
    "            # Store the adjusted R², AIC, and BIC for each subset\n",
    "            results.append({\n",
    "                'features': combo,\n",
    "                'adjusted_r2': model.rsquared_adj,\n",
    "                'AIC': model.aic,\n",
    "                'BIC': model.bic\n",
    "            })\n",
    "    \n",
    "    # Convert results to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Get the models with the best adjusted R², AIC, and BIC\n",
    "    best_adj_r2_model = results_df.loc[results_df['adjusted_r2'].idxmax()]\n",
    "    best_aic_model = results_df.loc[results_df['AIC'].idxmin()]\n",
    "    best_bic_model = results_df.loc[results_df['BIC'].idxmin()]\n",
    "    \n",
    "    return best_adj_r2_model, best_aic_model, best_bic_model\n",
    "#\n",
    "best_adj_r2_model, best_aic_model, best_bic_model = best_subsets_selection(X, y)\n",
    "\n",
    "print(\"Best model based on adjusted R²:\", best_adj_r2_model)\n",
    "print(\"Best model based on AIC:\", best_aic_model)\n",
    "print(\"Best model based on BIC:\", best_bic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 18\n",
    "## Best model found using all subsets selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 19\n",
    "## Summarize the models found using the different selection techniques and pick a best model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
